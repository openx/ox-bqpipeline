
<!doctype html>

<html>
<head>
  <meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1.0, user-scalable=yes">
  <meta name="theme-color" content="#4F7DC9">
  <meta charset="UTF-8">
  <title>Analytics Codelab: Scheduling Queries with your Team Git Repo</title>
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Source+Code+Pro:400|Roboto:400,300,400italic,500,700|Roboto+Mono">
  <link rel="stylesheet" href="//fonts.googleapis.com/icon?family=Material+Icons">
  <link rel="stylesheet" href="https://storage.googleapis.com/codelab-elements/codelab-elements.css">
  <style>
    .success {
      color: #1e8e3e;
    }
    .error {
      color: red;
    }
  </style>
</head>
<body>
  <google-codelab-analytics gaid="UA-49880327-14"></google-codelab-analytics>
  <google-codelab codelab-gaid="UA-52746336-1"
                  id="openx-bq-scheduling"
                  title="Analytics Codelab: Scheduling Queries with your Team Git Repo"
                  environment="web"
                  feedback-link="https://github.com/googlecodelabs/your-first-pwapp/issues">
    
      <google-codelab-step label="Introduction" duration="0">
        <p><strong>Last Updated:</strong> 2019-05-02</p>
<h2 is-upgraded><strong>What you&#39;ll build</strong></h2>
<p>In this codelab, you&#39;re going to build a BigQuery pipeline and schedule it with cron. </p>
<p>Your app will:</p>
<ul>
<li>Schedule a query using github</li>
<li>Export bigquery table data to a GCS bucket from a BigQuery pipeline</li>
</ul>
<h2 class="checklist" is-upgraded><strong>What you&#39;ll learn</strong></h2>
<ul class="checklist">
<li>How to author BigQuery Pipelines in python from the cloud shell</li>
<li>How to schedule a bigquery cron job</li>
<li>The basics of version controlling your code and collaborate with your team on github.</li>
</ul>
<p>This codelab is focused on running example queries to demo command line and query schedule features</p>
<h2 is-upgraded><strong>What you&#39;ll need</strong></h2>
<ul>
<li>A recent version of Chrome on your local machine</li>
<li>A Github account</li>
<li>Basics of github concepts</li>
<li>Knowledge of basic linux and gcloud commands</li>
</ul>


      </google-codelab-step>
    
      <google-codelab-step label="Cloud Shell Development Environment" duration="15">
        <p>Click the below button to get started</p>
<p><a href="https://console.cloud.google.com/cloudshell/editor?cloudshell_git_repo=https://github.com/openx/bq-analysts-team-bq-scheduled-jobs.git" target="_blank"><paper-button class="colored" raised>Cloud Shell</paper-button></a></p>
<h2 is-upgraded><strong>First time only setup</strong></h2>
<p>The quickest way to setup github authentication is to enter your Github username. Follow the instructions in <a href="https://medium.com/@ginnyfahs/github-error-authentication-failed-from-command-line-3a545bfd0ca8" target="_blank">this post</a> to create a personal access token and paste this as your password. You can name this personal access token <code>could-shell</code>.</p>
<p>Alternatively, to avoid having to do that every time you can configure passwordless login with an ssh key. </p>
<ul>
<li>Follow these <a href="https://help.github.com/en/articles/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent" target="_blank">instructions for creating a SSH key</a>. </li>
<li>Follow these <a href="https://help.github.com/en/articles/adding-a-new-ssh-key-to-your-github-account" target="_blank">instructions for adding an SSH key to your Github Account</a>. Note that <code>xclip</code> will not work for copying your public key. Instead just <code>cat</code>, highlight, copy and paste it into the browser.</li>
</ul>
<pre><code>chmod +r ~/.ssh/id_rsa.pub
cat ~/.ssh/id_rsa.pub
# Copy the output.
chmod -r ~/.ssh/id_rsa.pub</code></pre>
<p>Additionally, there&#39;s a one-time setup step for git to associate your username and email address with commits:</p>
<pre><code>git config --global user.email &#34;first.last@openx.com&#34;
git config --global user.name &#34;First Last&#34;</code></pre>
<p>Create a <a href="https://docs.python.org/3/library/venv.html" target="_blank">virtual environment</a> using the steps below. This will keep your python dependency management simpler.</p>
<pre><code>cd python
sudo apt-get install python3-venv -y &amp;&amp; \
python3 -m venv bq-pipeline-venv</code></pre>
<p>Activate your virtual environment.</p>
<pre><code>source bq-pipeline-venv/bin/activate</code></pre>
<p>Install dependencies defined in <code>requirements.txt</code>. We&#39;ve included several tools make your development workflow better. </p>
<p>If further python dependencies are needed this file will need to be updated.</p>
<pre><code>python3 -m pip install -r requirements.txt</code></pre>
<p>Note: You can safely ignore the warnings on &#34;bdlist_wheel&#34;</p>
<h2 is-upgraded><strong>Each Next Login</strong></h2>
<p>You&#39;re good to go! Next time you log into the cloud shell, you can simply activate your virtual environment and install again to ensure your installed packages are up to date </p>
<pre><code>source bq-pipeline-venv/bin/activate
python3 -m pip install -r requirements.txt</code></pre>


      </google-codelab-step>
    
      <google-codelab-step label="Git Branching" duration="5">
        <p>Create a <a href="https://git-scm.com/book/en/v2/Git-Branching-Branches-in-a-Nutshell" target="_blank">branch</a> for your development.</p>
<pre><code>git checkout -b feature/myNewScheduledQuery-$(whoami)</code></pre>
<ul>
<li>When you create a git branch you have an isolated place to work without disturbing your teammates.</li>
<li>Typically you should create a <a href="https://www.atlassian.com/git/tutorials/comparing-workflows/feature-branch-workflow" target="_blank">feature branch</a> for each new pipeline / scheduled job you are authoring. This branch should contain only your development on a single feature.</li>
<li>Once your feature is ready, push your changes to the git repository and create a <a href="https://help.github.com/en/articles/creating-a-pull-request" target="_blank">pull request</a> through the github UI or by following the link emitted by the initial <code>git push</code> (shown below). This where you will request a review from a teammate.</li>
</ul>
<pre><code>git add . 
git commit --all --message=&#39;this message describes the change&#39;
git push --set-upstream origin feature/myNewScheduledQuery-$(whoami)

# Contained in the output of this push should be a shortcut to create a pull request (PR). It will look like the following:
remote: Create a pull request for &#39;feature/myNewScheduledQuery-&lt;your-username&gt;&#39; on GitHub by visiting:
remote:      https://github.com/openx/analytics-team-repo/pull/new/feature/myNewScheduledQuery-brandonjacob</code></pre>
<ul>
<li>Once your change is approved, you will merge your feature branch into the <code>devint</code> branch. If you&#39;ve added a cron job it will get picked up and scheduled on the cron box auto-magically! We call this Continuous Deployment.</li>
</ul>


      </google-codelab-step>
    
      <google-codelab-step label="Git Repo Directory Structure" duration="5">
        <h2 is-upgraded><strong>Directory structure of the pipeline</strong></h2>
<p>Each team shares a repo to scheduled pipelines, and the repo comprises of the following elements:</p>
<ul>
<li><code>sql/</code> directory to define SQL Queries. This can optionally use jinja templating in the queries.</li>
<li><code>python/</code> directory to define Python wrappers. This should take advantage of the BQPipeline utility class in this repo. (<a href="https://github.com/openx/analytics-team-repo/blob/master/python/bqpipeline/README.md" target="_blank">docs</a>)</li>
<li>Python wrappers should reference sql file(s). If the file uses jinja templating the wrapper should provide the replacements.</li>
<li>A <code>crontab</code> file in the root of the repo to define your cron entry. This should reference your Python wrapper script.</li>
</ul>


      </google-codelab-step>
    
      <google-codelab-step label="Define Your First BigQuery Pipeline" duration="30">
        <p>In this section you will define a BigQuery Pipeline that does the following:</p>
<ul>
<li>Runs a query to define a <code>races</code> table containing information about the 2012 Olympic 800m championship race in a nested structure.</li>
<li>Runs a second query to unnest and perform an aggregation of 200m split times to finishing times and saves in the <code>finish_times</code> table.</li>
<li>Your pipeline exports the <code>finish_times</code> table to a CSV file on GCS.</li>
<li>Your pipeline deletes the BigQuery tables it created.</li>
</ul>
<h2 is-upgraded><strong>Define some SQL Scripts</strong></h2>
<p>First let&#39;s create a two sql files <code>race_splits.sql</code> and <code>finish_times.sql</code></p>
<ul>
<li>Note the <code>&#123;&#123; project }}</code> and <code>&#123;&#123; dataset }}</code> in the second query are <a href="https://realpython.com/primer-on-jinja-templating/" target="_blank">Jinja</a> templates. These values will be replaced by your python wrapper.</li>
</ul>
<p>Ensure you are in the <code>analytics-team-repo</code> directory (the repository root). Create a files <code>./sql/race_splits.sql</code> and <code>./sql/finish_times.sql:</code></p>
<pre>touch ./sql/race_splits.sql ; cloudshell edit ./sql/race_splits.sql
touch ./sql/finish_times.sql ; cloudshell edit ./sql/finish_times.sql</pre>
<p>Copy the contents of <code>./sql/race_splits.sql</code> to be:</p>
<pre># ./sql/race_splits.sql
SELECT &#34;800M&#34; AS race,
   [STRUCT(&#34;Rudisha&#34; as name, [23.4, 26.3, 26.4, 26.1] as splits),
    STRUCT(&#34;Makhloufi&#34; as name, [24.5, 25.4, 26.6, 26.1] as splits),
    STRUCT(&#34;Murphy&#34; as name, [23.9, 26.0, 27.0, 26.0] as splits),
    STRUCT(&#34;Bosse&#34; as name, [23.6, 26.2, 26.5, 27.1] as splits),
    STRUCT(&#34;Rotich&#34; as name, [24.7, 25.6, 26.9, 26.4] as splits),
    STRUCT(&#34;Lewandowski&#34; as name, [25.0, 25.7, 26.3, 27.2] as splits),
    STRUCT(&#34;Kipketer&#34; as name, [23.2, 26.1, 27.3, 29.4] as splits),
    STRUCT(&#34;Berian&#34; as name, [23.7, 26.1, 27.0, 29.3] as splits),
    STRUCT(&#34;Nathan&#34; as name, ARRAY&lt;FLOAT64&gt;[] as splits),
    STRUCT(&#34;David&#34; as name, NULL as splits)]
    AS participants</pre>
<p>Copy the contents of <code>./sql/finish_times.sql</code> to be:</p>
<pre># ./sql/finish_times.sql
SELECT
  name, sum(duration) AS finish_time
FROM \`&#123;&#123; project }}.&#123;&#123; dataset }}\`.races, races.participants LEFT JOIN participants.splits duration
GROUP BY name;</pre>
<h2 is-upgraded>Define a Python Wrapper</h2>
<p>Start by making a copy of the <code>example.py</code> and opening it in your editor<code>:</code></p>
<pre><code>cd python
cp example_pipeline.py my_first_pipeline.py
cloudshell edit my_first_pipeline.py</code></pre>
<p> Now we&#39;ll walk through line by line and edit  <code>my_first_pipeline.py</code> in the Cloud Shell Code Editor:</p>
<ul>
<li>Edit the docstring in line 4 to describe your pipeline.</li>
<li>The JOB_NAME and DATASET should be changed to meet your job&#39;s configuration:</li>
</ul>
<pre><code> ...
    JOB_NAME = &#34;race-analysis&#34;
    DATASET = &#34;scratch&#34;
...</code></pre>
<ul>
<li>Notice the <code>BQP</code> object is of class <code>BQPipeline</code>. This provides us convenience methods for interacting with BigQuery.</li>
<li>Search for variable assignments to <code>TBL1, TBL2,</code> etc. We&#39;re only working with two tables for this scheduled job, so delete all but <code>TBL1 and TBL2</code>. Both should be set to <code>= races</code></li>
<li>In <code>BQP.run_queries</code> we pass a list of (query, destination table) pairs </li>
<li>Edit the first entry to invoke <code>race_splits.sql</code> into destination table <code>races</code></li>
<li>Edit the second entry to invoke <code>finish_times.sql</code> into GCS destination: <code>&#39;gs://analytics-scratch/csv_export/finish_times/&#39;</code></li>
<li>Note , the framework will create a `runtime=YYYY-MM-DDTHH:MM:SS/` directory and dump the export files here named as `&lt;job-name&gt;-*.csv` where the * handles splitting files if the export is &gt; 1GB.</li>
<li>Note that tables can be passed as </li>
<li><code>table_id</code> (project and dataset inferred from BQP)</li>
<li><code>dataset_id.table_id</code> (project inferred from BQP)</li>
<li><code>Project_id.dataset_id.table_id</code> (fully qualified path)</li>
<li>Edit the <code>BQP.delete_tables</code> expression to delete the <code>races</code> table.</li>
</ul>
<h2 is-upgraded><strong>Edit the crontab file</strong></h2>
<p>In the cloud shell editor, edit the <code>crontab</code> file at the root of this directory to get your pipeline scheduled. Each  entry consist of:</p>
<ul>
<li>A Cron Schedule string. Adhere to <a href="https://en.wikipedia.org/wiki/Cron#Overview" target="_blank">cron schedule syntax</a> and Use <a href="https://crontab.guru/" target="_blank">cron guru</a> to check your schedule string</li>
<li>Invoke python3</li>
<li>Fully qualified path to your pipeline script. (Note: these should all start with <code>/home/ubuntu/cronjob_repo/python/)</code></li>
</ul>
<pre>*/30 * * * * python3 /home/ubuntu/cronjob_repo/python/example_pipeline.py</pre>


      </google-codelab-step>
    
      <google-codelab-step label="Preparing an A&#43; PR for schedule Job" duration="30">
        <h2 is-upgraded><strong>Linting your sql</strong></h2>
<p>Format your query using <code>sqlformat.</code> This tool can automate making your sql queries look pretty and consistent, similar to the bigquery format button in the UI. If you&#39;ve never used this tool within cloudshell, install it using:</p>
<pre><code>sudo apt-get install sqlformat -y</code></pre>
<p>Invoke <code>sqlformat</code> from the <code>sql/</code> directory:</p>
<pre>sqlformat \
  --reindent \
  --keywords upper \
  --identifiers lower \
  race_splits.sql -o race_splits.sql

sqlformat \
  --reindent \
  --keywords upper \
  --identifiers lower \
  finish_times.sql -o finish_times.sql</pre>
<h2 is-upgraded><strong>Validate your query</strong></h2>
<ul>
<li>Passing the <code>--dry_run</code> flag will quickly validate syntax and check that the tables you have referenced exist</li>
<li>If you&#39;d actually like to run the query just omit the <code>--dry_run</code> flag</li>
</ul>
<pre>cat race_splits.sql | bq query --use_legacy_sql=false --dry_run

cat finish_times.sql | bq query --use_legacy_sql=false --dry_run </pre>
<ul>
<li>Note, this second dry run will fail for a few reasons </li>
<li>There are Jinja templates aren&#39;t rendered</li>
<li>The races table doesn&#39;t exist because this table should be created by the first query which never actually ran</li>
</ul>
<p>Successfully validating this query</p>
<pre># Create this table in the scratch dataset. (Not necessary for existing tables)
bq query --use_legacy_sql=false \
--destination_table=ox-data-analytics-prod.scratch.races \
race_splits.sql 

# Set environment variables for templated values.
export project=ox-data-analytics-prod
export dataset=scratch

# Use j2 to render the template and dry run it.
j2 finish_times.sql | bq query --use_legacy_sql=false --dry_run</pre>
<p>Check your Python style. Note the cloud shell code editor will give you hints on python style as you develop! <code>pylint</code> is just a CLI tool to use as a final check.</p>
<h3 is-upgraded>Strongly Recommended: Strive to achieve 10/10 score ! </h3>
<pre><code>python3 -m pylint my_first_pipeline.py</code></pre>
<h3 is-upgraded>Validate your python wrapper by running the below command locally and observe the expected logs and outputs in GCP console. Note, this is not a dry run, each query in you pipeline will actually get executed in BigQuery and destination tables created as needed.</h3>
<h3 is-upgraded><code>python3 my_first_pipeline.py</code></h3>
<p><strong>Submitting a git Pull Request</strong></p>
<p>Use below git commands to commit and push your code (checkout this <a href="https://try.github.io/" target="_blank">reference for learning git!</a>):</p>
<pre><code>git add sql/ python/ crontab
git commit -m &#34;Adding &lt;name of your scheduled job&gt;&#34;
git push --set-upstream origin feature/myNewScheduledQuery</code></pre>
<h3 is-upgraded>Strongly Recommended: <strong>Please do not add / commit your virtual environment to the repo!</strong></h3>
<p>You can now <a href="https://help.github.com/en/articles/creating-a-pull-request" target="_blank">create a Pull Request</a> in your browser with Github.</p>


      </google-codelab-step>
    
      <google-codelab-step label="How your Job Gets Scheduled" duration="1">
        <p>Once your PR is merged to this repo your work is done.</p>
<p>The scheduling VM pulls this repo once every 15 mins. This means that you may need to wait up to 15 mins before your first scheduled run occurs. </p>
<p>Note, merging to the devint branch is effectively deploying those change in the devint project. </p>


      </google-codelab-step>
    
      <google-codelab-step label="Monitoring your Job" duration="3">
        <p>The BigQuery Pipeline utility class is configured to log to files in <code>/home/ubuntu/bq-pipeline-*.log*</code>. The CronBox is configured with <a href="https://www.fluentd.org/" target="_blank">fluentd</a> to write logs in this directory to StackDriver and tags each log with <code>bq-analyst-cron</code>.</p>
<p>You can look at the logs with this command. Using the <code>--freshness</code> flag to control how far back in the logs you want to look.</p>
<pre><code>gcloud logging read \
--freshness 3h \
&#39;logName=&#34;projects/ox-data-analytics-devint/logs/bq-analyst-cron&#34;&#39;</code></pre>


      </google-codelab-step>
    
      <google-codelab-step label="Conclusion" duration="1">
        <h3 is-upgraded>In this code lab we learned</h3>
<ul>
<li>How to set up a Cloud Shell development environment</li>
<li>How to write a Python BigQuery Pipeline that references sql files</li>
<li>How to ensure high code quality</li>
<li>The basics of using github</li>
</ul>
<p>Once your Pull Request is merged to the branch for the appropriate environment, it will be scheduled on that environment&#39;s BigQuery CronBox VM. This means the process for promoting from devint to qa is to create a Pull Request that merges the devint branch to qa.</p>


      </google-codelab-step>
    
  </google-codelab>

  <script src="https://storage.googleapis.com/codelab-elements/native-shim.js"></script>
  <script src="https://storage.googleapis.com/codelab-elements/custom-elements.min.js"></script>
  <script src="https://storage.googleapis.com/codelab-elements/prettify.js"></script>
  <script src="https://storage.googleapis.com/codelab-elements/codelab-elements.js"></script>

</body>
</html>
